name: Model Training Pipeline

# Scheduled training with manual trigger option
# Runs weekly on Sunday 2 AM UTC

on:
  schedule:
    - cron: '0 2 * * 0'  # Every Sunday at 2 AM UTC
  workflow_dispatch:
    inputs:
      optimize:
        description: 'Run hyperparameter optimization first'
        required: false
        default: false
        type: boolean
      reason:
        description: 'Reason for manual trigger'
        required: false
        default: 'Manual training request'

env:
  PYTHON_VERSION: '3.12'

jobs:
  train:
    name: Train Model
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run hyperparameter optimization
        if: ${{ github.event.inputs.optimize == 'true' }}
        run: |
          echo "üîß Running hyperparameter optimization..."
          python scripts/optimize_hyperparams.py --trials 50 --timeout 1800
        continue-on-error: true

      - name: Train production model
        id: train
        run: |
          echo "üöÇ Training production model..."
          mkdir -p models reports
          python scripts/train_production.py \
            --output-json training_results.json \
            2>&1 | tee training_output.log
          
          # Extract metrics from output
          if grep -q "TRAINING COMPLETE" training_output.log; then
            echo "training_success=true" >> $GITHUB_OUTPUT
          else
            echo "training_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Validate new model
        if: steps.train.outputs.training_success == 'true'
        id: validate
        run: |
          echo "üîç Validating new model..."
          mkdir -p reports
          python scripts/validate_model.py \
            --staging \
            --compare-production \
            --output-report reports/model_comparison.md \
            2>&1 | tee validation_output.log
          
          if grep -q "VALIDATION PASSED" validation_output.log; then
            echo "should_promote=true" >> $GITHUB_OUTPUT
          else
            echo "should_promote=false" >> $GITHUB_OUTPUT
          fi

      - name: Promote model to production
        if: steps.validate.outputs.should_promote == 'true'
        run: |
          echo "‚úÖ Promoting model to production..."
          python scripts/version_model.py promote \
            --from staging \
            --to production \
            --bump patch

      - name: Upload model artifacts
        uses: actions/upload-artifact@v6
        with:
          name: model-${{ github.run_number }}
          path: |
            models/staging/
            reports/
            training_output.log
          retention-days: 30

      - name: Upload comparison report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: validation-report-${{ github.run_number }}
          path: reports/model_comparison.md
          retention-days: 90

      - name: Training Summary
        if: always()
        run: |
          echo "## üìä Training Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Trigger | ${{ github.event_name }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Run Number | ${{ github.run_number }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Training Success | ${{ steps.train.outputs.training_success }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Promoted | ${{ steps.validate.outputs.should_promote }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f reports/model_comparison.md ]; then
            echo "### Model Comparison" >> $GITHUB_STEP_SUMMARY
            cat reports/model_comparison.md >> $GITHUB_STEP_SUMMARY
          fi

  notify:
    name: Send Notification
    runs-on: ubuntu-latest
    needs: train
    if: always()
    
    steps:
      - name: Notify on success
        if: needs.train.result == 'success'
        run: |
          echo "‚úÖ Model training pipeline completed successfully!"
          echo "Run: ${{ github.run_number }}"

      - name: Notify on failure
        if: needs.train.result == 'failure'
        run: |
          echo "‚ùå Model training pipeline failed!"
          echo "Check logs for details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
